[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/A1.html",
    "href": "posts/A1.html",
    "title": "강화학습 (1) – bandit",
    "section": "",
    "text": "{{&lt;video https://youtu.be/playlist?list=PLQqh36zP38-zoOHd7w3N5q9Jc5P34Ux8X&si=MdJTHM3a27MCAssp &gt;}}"
  },
  {
    "objectID": "posts/A1.html#footnotes",
    "href": "posts/A1.html#footnotes",
    "title": "강화학습 (1) – bandit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nbutton1을 눌러야 하는건 맞지만 20번에 한번정도의 실수는 눈감아 주는 조건↩︎\nbutton1을 누른다↩︎"
  },
  {
    "objectID": "posts/A3.html",
    "href": "posts/A3.html",
    "title": "강화학습 (3) – LunarLander",
    "section": "",
    "text": "강의영상\n{{&lt;video https://youtu.be/playlist?list=PLQqh36zP38-zBEizLbjgRE8qMfsJML6Ua&si=HALKE6fjiWB12AGW &gt;}}\n\n\nimports\n\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport torch\nimport collections\nimport IPython\n\n\n\n예비학습\n- collections.deque 의 기능\n\na = collections.deque([1,2,3], maxlen = 5 )\na\n\ndeque([1, 2, 3], maxlen=5)\n\n\n\na.append(4)\na\n\ndeque([1, 2, 3, 4], maxlen=5)\n\n\n\na.append(5)\na\n\ndeque([1, 2, 3, 4, 5], maxlen=5)\n\n\n\na.append(6)\na\n\ndeque([2, 3, 4, 5, 6], maxlen=5)\n\n\n- 단점? numpy array 보다는 list 느낌임 (연산에 특화된건 아님)\n\na + 1\n\nTypeError: can only concatenate deque (not \"int\") to deque\n\n\n- 그렇지만 필요하다면 np.array 화 시킬 수 있음.\n\nnp.array(a) + 1\n\narray([3, 4, 5, 6, 7])\n\n\n- collection.deque 는 리플레이 버퍼를 구현할때 유용한 자료구조이다.\n\n(우리가 했던) 기존방식: 모든 데이터를 저장하며 하나의 경험씩 학습함\n리플레이버퍼: 최근 \\(N\\)개의 데이터를 저장하여 여러경험을 샘플링하여 학습하는 방식\n리플레이버퍼의 장점: 메모리를 아낄 수 있다, 다양한 종류의 경험을 저장하고 무작위로 재사용하여 학습이 안정적으로 된다, “저장 -&gt; 학습 -&gt; 저장” 순으로 반드시 실시간으로 학습할 필요가 없어서 병렬처리에 용이하다, 강화학습에서 연속된 경험은 상관관계가 있을 수 있는데 무작위 샘플로 이러한 상관관계를 제거할 수 있음\n\n\n\nGame3: LunarLander\n- 환경생성\n\nenv = gym.make('LunarLander-v2', render_mode = 'rgb_array') \nenv \n\n&lt;TimeLimit&lt;OrderEnforcing&lt;PassiveEnvChecker&lt;LunarLander&lt;LunarLander-v2&gt;&gt;&gt;&gt;&gt;\n\n\n- state_space\n\nenv.observation_space\n\nBox([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n 1.       ], (8,), float32)\n\n\n- action_space\n\nenv.action_space\n\nDiscrete(4)\n\n\n- env.reset()\n\nstate, _ = env.reset()\nstate \n\narray([-0.004881  ,  1.4137907 , -0.49441272,  0.12757756,  0.00566272,\n        0.11199194,  0.        ,  0.        ], dtype=float32)\n\n\n- env.render()\n\nplt.imshow(env.render())\n\n&lt;matplotlib.image.AxesImage at 0x7f9a5477bb10&gt;\n\n\n\n\n\n- env.step\n\nnext_state, reward, terminated, _, _ = env.step(0)\nnext_state, reward, terminated\n\n(array([-0.00976257,  1.4160839 , -0.4937438 ,  0.10189002,  0.01119673,\n         0.11069117,  0.        ,  0.        ], dtype=float32),\n -0.13923681373518093,\n False)\n\n\n- play\n\nenv.reset()\nplt.imshow(env.render())\n\n&lt;matplotlib.image.AxesImage at 0x7f9a5349a410&gt;\n\n\n\n\n\n\nfor _ in range(7):\n    env.step(3)\n    env.step(2)\nplt.imshow(env.render())\n\n&lt;matplotlib.image.AxesImage at 0x7f9a534ea410&gt;\n\n\n\n\n\n\n0 : 아무행동도 하지 않음\n1 : 왼쪽\n2 : 위\n3 : 오른쪽\n\n\n\n시각화\n\ndef show(ims,jump=10):\n    ims = ims[::jump]\n    fig = plt.Figure()\n    ax = fig.subplots()\n    def update(i):\n       ax.imshow(ims[i])\n    ani = FuncAnimation(fig,update,frames=len(ims))\n    display(IPython.display.HTML(ani.to_jshtml()))\n\n\ncurrent_state, _ = env.reset()\nims = [] \nfor t in range(500): \n    action = env.action_space.sample()\n    next_state, reward, terminated, _, _ = env.step(action)\n    im = env.render()\n    ims.append(im) \n    current_state = next_state \n    if terminated: break \n\n\nshow(ims) \n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nq_net\n- 원래는 agent.q 에 해당하는 것인데, 이전에서는 agent.q를 (4,4,4) shape의 numpy array 를 사용했는데 여기서는 불가능\n\n4x4 grid: 상태공간의 차원은 2차원이며 가질수 있는 값은 16개, 각 상태공간에서 할수 있는 행동이 4개 -&gt; 총 16*4의 경우의 수에 대한 reward만 조사하면 되었음\nLunarLander: 상태공간의 차원은 8차원이지만 가질수 있는 값의 범위는 무한대 -&gt; 무수히 많은 경우에 대한 reward 값을 조사하는건 현실적으로 불가능\n\n- 데이터를 모아보자.\n\ncurrent_states = collections.deque(maxlen=50) \nactions = collections.deque(maxlen=50) \nnext_states = collections.deque(maxlen=50) \nrewards = collections.deque(maxlen=50) \nterminations = collections.deque(maxlen=50) \n\ncurrent_state, _ = env.reset()\nfor t in range(500): \n    ## step1: agent &gt;&gt; env \n    action = env.action_space.sample()\n    ## step2:agent &lt;&lt; env \n    next_state, reward, terminated, _, _ = env.step(action)\n    current_states.append(current_state)\n    actions.append(action)\n    next_states.append(next_state)\n    rewards.append(reward)\n    terminations.append(terminated) \n    ## step3: learn \n    ## step4: update state     \n    current_state = next_state \n    ## step5: 종료조건체크 \n    if terminated: break \n\n- 이전코드에서 아래에 대응하는 부분을 구현하면 된다.\n## 1. q[x,y,a]를 초기화: q(s)를 넣으면 action에 대한 q값을 알려주는 기능 \nagent.q = np.zeros([4,4,4]) \n\n## 2. q_estimated 를 계산 \nx,y = agent.current_state\nxx,yy = agent.next_state\na = agent.action \nq_estimated = agent.q[x,y,a] \n\n## 3. q_realistic = agent.reward + 0.99 * q_future 를 수행하는 과정 \nif agent.terminated:\n    q_realistic = agent.reward\nelse:\n    q_future = q[xx,yy,:].max()\n    q_realistic = agent.reward + 0.99 * q_future\n\n## 4. q_estimated 를 점점 q_realistic 와 비슷하게 만드는 과정 \ndiff = q_realistic - q_estimated \nagent.q[x,y,a] = q_estimated + 0.05 * diff \n1. agent.q 에 대응하는 과정\n\nq_net = torch.nn.Sequential(\n    torch.nn.Linear(8,128),\n    torch.nn.ReLU(),\n    torch.nn.Linear(128,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,4)\n)\n\n\nq_net # &lt;- 8개의 숫자가 입력으로 오면 4개의 숫자를 리턴하는 함수 \n\nSequential(\n  (0): Linear(in_features=8, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=32, bias=True)\n  (5): ReLU()\n  (6): Linear(in_features=32, out_features=4, bias=True)\n)\n\n\n\nq_net(torch.tensor(current_state))\n\ntensor([-0.0863, -0.0824, -0.1490,  0.0031], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nq_net은 8개의 숫자가 입력으로 오면 4개의 숫자가 리턴되는 함수이다.\n해석을 하면 8개의 숫자는 state를 나타내는 숫자로 이해할 수 있고 4개의 숫자는 각 action에 대한 q값으로 해석할 수 있다.\n하지만 이 숫자가 합리적인건 아님 (아무숫자임)\nq_net의 특징: 고정된 함수가 아니고 데이터를 이용하여 점점 더 그럴듯한 숫자를 뱉어내도록 학습할 수 있는 함수이다. (뉴럴네트워크)\n\n1. agent.q 에 대응하는 과정 (배치버전)\n– get batch –\n\nbatch_size = 4 \nidx = np.random.randint(0,50,size=batch_size)\n\ncurrent_states_batch = torch.tensor(np.array(current_states))[idx].float()\nactions_batch = torch.tensor(np.array(actions))[idx].reshape(batch_size,-1) \nrewards_batch = torch.tensor(np.array(rewards))[idx].reshape(batch_size,-1).float()\nnext_states_batch = torch.tensor(np.array(next_states))[idx].float()\nterminations_batch = torch.tensor(np.array(terminations))[idx].reshape(batch_size,-1)\n\n– q_net –\n\ncurrent_states_batch\n\ntensor([[-0.5863,  0.7144, -0.7831, -1.1050,  0.0357, -0.0844,  0.0000,  0.0000],\n        [-0.4805,  1.0306, -0.7311, -0.8693,  0.1304, -0.1544,  0.0000,  0.0000],\n        [-0.6180,  0.6100, -0.7990, -1.1882,  0.0206, -0.0456,  0.0000,  0.0000],\n        [-0.6100,  0.6367, -0.7883, -1.1610,  0.0229, -0.0884,  0.0000,  0.0000]])\n\n\n\nq_net(current_states_batch)\n\ntensor([[-0.0974, -0.0949, -0.0918,  0.0467],\n        [-0.1009, -0.1039, -0.0828,  0.0529],\n        [-0.0953, -0.0925, -0.0947,  0.0437],\n        [-0.0961, -0.0925, -0.0942,  0.0443]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n2. q_estimated\n\nq_net(current_states_batch), actions_batch\n\n(tensor([[-0.0974, -0.0949, -0.0918,  0.0467],\n         [-0.1009, -0.1039, -0.0828,  0.0529],\n         [-0.0953, -0.0925, -0.0947,  0.0437],\n         [-0.0961, -0.0925, -0.0942,  0.0443]], grad_fn=&lt;AddmmBackward0&gt;),\n tensor([[0],\n         [1],\n         [3],\n         [1]]))\n\n\n\nq_net(current_states_batch).gather(1,actions_batch)\n\ntensor([[-0.0974],\n        [-0.1039],\n        [ 0.0437],\n        [-0.0925]], grad_fn=&lt;GatherBackward0&gt;)\n\n\n3. q_realistic = agent.reward + 0.99 * q_future\n– q_future –\n\nq_future = q_net(next_states_batch).max(axis=1)[0].reshape(batch_size,1)\nq_future\n\ntensor([[0.0461],\n        [0.0538],\n        [0.0421],\n        [0.0437]], grad_fn=&lt;ReshapeAliasBackward0&gt;)\n\n\n\nq_realistic = rewards_batch + 0.99 * q_future * (~terminations_batch)\n\n4. q_estimated 를 점점 q_realistic 와 비슷하게 만드는 과정\n## 여기는.. 딥러닝과 파이토치를 좀 알아야.. 모른다면 일단 패스해야합니다.. \noptimizer = torch.optim.Adam(q_net.parameters(),lr=0.0001) \nfor _ in range(2000):\n    ~~~\n    ~~~\n    q_estimated = ~~~ \n    q_realistic = ~~~ \n    loss = torch.nn.functional.mse_loss(q_estimated,q_realistic)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n\npolicy\n\neps = 0.5 \nif np.random.rand() &lt; eps:\n    action = env.action_space.sample() \nelse:\n    action = q_net(torch.tensor(current_state)).argmax().item()\n\n\naction\n\n3\n\n\n\n\nAgent 클래스 + run\n\nclass Agent():\n    def __init__(self,env):\n        self.eps = 0\n        self.n_experiences = 0\n        self.n_episode = 0\n        self.score = 0\n        self.scores = []\n        self.playtimes = []\n        self.batch_size = 64\n        self.buffer_size = 5000 \n        self.action_space = env.action_space\n        #self.state_space = env.observation_space\n\n        # Q-Network\n        self.q_net = torch.nn.Sequential(\n            torch.nn.Linear(8,128), \n            torch.nn.ReLU(),\n            torch.nn.Linear(128,64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64,32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32,4)\n        ) \n        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=0.0001)\n\n        # ReplayBuffer\n        self.current_states = collections.deque(maxlen=self.buffer_size)\n        self.actions = collections.deque(maxlen=self.buffer_size)\n        self.rewards = collections.deque(maxlen=self.buffer_size)\n        self.next_states = collections.deque(maxlen=self.buffer_size)\n        self.terminations = collections.deque(maxlen=self.buffer_size)\n       \n    def save_experience(self):\n        \"\"\"Add a new experience to memory.\"\"\"\n        self.current_states.append(self.current_state)\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.next_states.append(self.next_state)\n        self.terminations.append(self.terminated) \n        self.n_experiences = self.n_experiences+1\n        self.score += self.reward\n    \n    def act(self):\n        if np.random.rand() &lt; self.eps:\n            self.action = self.action_space.sample()\n        else:\n            self.action = self.q_net(torch.tensor(self.current_state)).argmax().item()\n            \n    def get_batch(self):\n        idx = np.random.randint(0,self.buffer_size,size=self.batch_size) \n        self.current_states_batch = torch.tensor(np.array(self.current_states))[idx].float()\n        self.actions_batch = torch.tensor(np.array(self.actions))[idx].reshape(self.batch_size,1)\n        self.rewards_batch = torch.tensor(np.array(self.rewards))[idx].reshape(self.batch_size,-1).float()\n        self.next_states_batch = torch.tensor(np.array(self.next_states))[idx].float()\n        self.terminations_batch = torch.tensor(np.array(self.terminations))[idx].reshape(self.batch_size,-1) \n    \n    def learn(self):\n        if self.n_experiences &lt; self.buffer_size:\n            pass\n        else: \n            self.get_batch()\n            q_estimated = self.q_net(self.current_states_batch).gather(1, self.actions_batch)\n            q_future = self.q_net(self.next_states_batch).detach().max(1)[0].reshape(self.batch_size,1)\n            q_realistic = self.rewards_batch + 0.99 * q_future * (~self.terminations_batch)\n\n            loss = torch.nn.functional.mse_loss(q_estimated, q_realistic)\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n\nenv = gym.make('LunarLander-v2',render_mode='rgb_array')\nagent = Agent(env)\nagent.eps = 1.0 \nfor _ in range(2000):\n    ### 1. 본질적인 코드\n    agent.current_state, _  = env.reset() \n    agent.terminated = False\n    agent.score = 0 \n    for t in range(500):\n        # step1: agent &gt;&gt; env \n        agent.act() \n        env.agent_action = agent.action  \n        # step2: agent &lt;&lt; env \n        agent.next_state, agent.reward, agent.terminated, _,_ = env.step(env.agent_action)\n        agent.save_experience() \n        # step3: learn \n        agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episode = agent.n_episode + 1 \n    agent.eps = agent.eps*0.995\n    ## 2. 비본질적 코드\n    if (agent.n_episode % 10) == 0:\n        print(\n            f'Episode {agent.n_episode}\\t'\n            f'Score: {np.mean(agent.scores[-100:]) : .2f}\\t'\n            f'Playtime: {np.mean(agent.playtimes[-100:]) : .2f}\\t'\n            f'n_eps: {agent.eps}\\t'\n            f'n_experiences: {agent.n_experiences}\\t'\n        )\n    if np.mean(agent.scores[-100:])&gt;=200.0:\n        break\n\nEpisode 10  Score: -213.18  Playtime:  92.70    n_eps: 0.9511101304657719   n_experiences: 927  \nEpisode 20  Score: -204.70  Playtime:  99.50    n_eps: 0.9046104802746175   n_experiences: 1990 \nEpisode 30  Score: -211.72  Playtime:  104.50   n_eps: 0.8603841919146962   n_experiences: 3135 \nEpisode 40  Score: -226.75  Playtime:  105.53   n_eps: 0.8183201210226743   n_experiences: 4221 \nEpisode 50  Score: -208.68  Playtime:  106.34   n_eps: 0.778312557068642    n_experiences: 5317 \nEpisode 60  Score: -197.43  Playtime:  108.47   n_eps: 0.7402609576967045   n_experiences: 6508 \nEpisode 70  Score: -208.33  Playtime:  115.60   n_eps: 0.7040696960536299   n_experiences: 8092 \nEpisode 80  Score: -212.00  Playtime:  117.41   n_eps: 0.6696478204705644   n_experiences: 9393 \nEpisode 90  Score: -208.73  Playtime:  118.74   n_eps: 0.6369088258938781   n_experiences: 10687    \nEpisode 100 Score: -206.49  Playtime:  119.74   n_eps: 0.6057704364907278   n_experiences: 11974    \nEpisode 110 Score: -196.32  Playtime:  123.45   n_eps: 0.5761543988830038   n_experiences: 13272    \nEpisode 120 Score: -184.32  Playtime:  129.69   n_eps: 0.547986285490042    n_experiences: 14959    \nEpisode 130 Score: -172.21  Playtime:  130.03   n_eps: 0.5211953074858876   n_experiences: 16138    \nEpisode 140 Score: -152.05  Playtime:  142.30   n_eps: 0.49571413690105054  n_experiences: 18451    \nEpisode 150 Score: -143.29  Playtime:  146.41   n_eps: 0.47147873742168567  n_experiences: 19958    \nEpisode 160 Score: -132.48  Playtime:  154.47   n_eps: 0.4484282034609769   n_experiences: 21955    \nEpisode 170 Score: -106.66  Playtime:  163.24   n_eps: 0.42650460709830135  n_experiences: 24416    \nEpisode 180 Score: -85.85   Playtime:  180.09   n_eps: 0.40565285250151817  n_experiences: 27402    \nEpisode 190 Score: -73.39   Playtime:  201.63   n_eps: 0.3858205374665315   n_experiences: 30850    \nEpisode 200 Score: -49.93   Playtime:  230.35   n_eps: 0.3669578217261671   n_experiences: 35009    \nEpisode 210 Score: -40.46   Playtime:  263.12   n_eps: 0.34901730169741024  n_experiences: 39584    \nEpisode 220 Score: -31.11   Playtime:  280.75   n_eps: 0.33195389135223546  n_experiences: 43034    \nEpisode 230 Score: -15.80   Playtime:  314.08   n_eps: 0.3157247089126454   n_experiences: 47546    \nEpisode 240 Score: -5.43    Playtime:  333.12   n_eps: 0.30028896908517405  n_experiences: 51763    \nEpisode 250 Score:  4.33    Playtime:  363.03   n_eps: 0.285607880564032    n_experiences: 56261    \nEpisode 260 Score:  10.80   Playtime:  391.83   n_eps: 0.27164454854530906  n_experiences: 61138    \nEpisode 270 Score:  14.88   Playtime:  413.84   n_eps: 0.2583638820072446   n_experiences: 65800    \nEpisode 280 Score:  21.58   Playtime:  432.86   n_eps: 0.2457325055235537   n_experiences: 70688    \nEpisode 290 Score:  31.61   Playtime:  443.43   n_eps: 0.23371867538818816  n_experiences: 75193    \nEpisode 300 Score:  29.04   Playtime:  439.61   n_eps: 0.22229219984074702  n_experiences: 78970    \nEpisode 310 Score:  37.79   Playtime:  443.86   n_eps: 0.21142436319205632  n_experiences: 83970    \nEpisode 320 Score:  43.76   Playtime:  456.21   n_eps: 0.2010878536592394   n_experiences: 88655    \nEpisode 330 Score:  43.98   Playtime:  461.09   n_eps: 0.1912566947289212   n_experiences: 93655    \nEpisode 340 Score:  45.86   Playtime:  468.92   n_eps: 0.18190617987607657  n_experiences: 98655    \nEpisode 350 Score:  50.36   Playtime:  473.94   n_eps: 0.1730128104744653   n_experiences: 103655   \nEpisode 360 Score:  51.94   Playtime:  467.59   n_eps: 0.16455423674261854  n_experiences: 107897   \nEpisode 370 Score:  51.95   Playtime:  467.90   n_eps: 0.15650920157696743  n_experiences: 112590   \nEpisode 380 Score:  55.56   Playtime:  469.02   n_eps: 0.14885748713096328  n_experiences: 117590   \nEpisode 390 Score:  62.16   Playtime:  473.97   n_eps: 0.14157986400593744  n_experiences: 122590   \nEpisode 400 Score:  71.21   Playtime:  485.75   n_eps: 0.1346580429260134   n_experiences: 127545   \nEpisode 410 Score:  82.20   Playtime:  479.03   n_eps: 0.12807462877562611  n_experiences: 131873   \nEpisode 420 Score:  96.33   Playtime:  473.30   n_eps: 0.12181307688414106  n_experiences: 135985   \nEpisode 430 Score:  115.43  Playtime:  466.74   n_eps: 0.11585765144771248  n_experiences: 140329   \nEpisode 440 Score:  118.52  Playtime:  463.05   n_eps: 0.11019338598389174  n_experiences: 144960   \nEpisode 450 Score:  117.20  Playtime:  463.05   n_eps: 0.10480604571960442  n_experiences: 149960   \nEpisode 460 Score:  135.12  Playtime:  460.61   n_eps: 0.0996820918179746   n_experiences: 153958   \nEpisode 470 Score:  156.37  Playtime:  447.12   n_eps: 0.09480864735409487  n_experiences: 157302   \nEpisode 480 Score:  175.40  Playtime:  433.66   n_eps: 0.09017346495423652  n_experiences: 160956   \nEpisode 490 Score:  191.98  Playtime:  417.28   n_eps: 0.08576489601717459  n_experiences: 164318   \n\n\n- 시각화를 위한코드\n\nagent2 = Agent(env) \nagent2.q_net = agent.q_net\n\nagent2.current_state, _ = env.reset()\nagent2.terminated = False \nims = [] \nims.append(env.render())\nfor t in range(500):\n    agent2.act() \n    agent2.next_state, agent2.reward, agent2.terminated, _, _  = env.step(agent2.action)\n    im = env.render()\n    ims.append(im)\n    agent2.current_state = agent2.next_state\n    if agent2.terminated: break \n\n\nshow(ims)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/A2.html",
    "href": "posts/A2.html",
    "title": "강화학습 (2) – 4x4 grid",
    "section": "",
    "text": "{{&lt;video https://youtu.be/playlist?list=PLQqh36zP38-zHvVuJ92xfdypwHwDFgg8k&si=iI4IhthblTsJTmIv &gt;}}"
  },
  {
    "objectID": "posts/A2.html#footnotes",
    "href": "posts/A2.html#footnotes",
    "title": "강화학습 (2) – 4x4 grid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n즉 next_state가 가지는 잠재적값어치는 고려되어있지 않음↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "test",
    "section": "",
    "text": "강화학습 (3) – LunarLander\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n강화학습 (2) – 4x4 grid\n\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n강화학습 (1) – bandit\n\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2023\n\n\n최규빈\n\n\n\n\n\n\nNo matching items"
  }
]